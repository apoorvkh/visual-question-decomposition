# ViperGPT Execution Engine

This repo is a derivative of the [original codebase](https://github.com/cvlab-columbia/viper) for [ViperGPT](https://viper.cs.columbia.edu). In particular, this repo attempts to faithfully implement ViperGPT's execution engine according to the [original paper](https://arxiv.org/abs/2303.08128v1).

If you use this code, please consider citing:

```
@inproceedings{suris2023:vipergpt,
    title     = {ViperGPT: Visual Inference via Python Execution for Reasoning},
    author    = {D\'idac Sur\'is and Sachit Menon and Carl Vondrick},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {11888-11898}
}
```

## Modifications

This repo exclusively features ViperGPT's execution engine. All code related to program generation and benchmarking has been removed â€” for the full pipeline (with VQA benchmarks), please consider using [brown-palm/visual-question-decomposition](https://github.com/brown-palm/visual-question-decomposition).

You can view this [code diff versus the original repo](https://github.com/brown-palm/visual-question-decomposition/compare/bde4c6343825e6a131547cdfdeed8a62c9ac4b11..viper) (from the time of modification). You can track more recent updates to the original repo [here](https://github.com/cvlab-columbia/viper/compare/bde4c6343825e6a131547cdfdeed8a62c9ac4b11..main).

- This repo only supports image manipulation. The `VideoSegment` class and relevant video processing modules have been removed.
- Models: GLIP, X-VLM, MiDaS, GPT-3, BLIP-2. All other models are not used in the [ViperGPT paper](https://arxiv.org/abs/2303.08128v1) and have consequently been removed.
- Execution: Code is executed line-by-line by parsing the [AST](https://en.wikipedia.org/wiki/Abstract_syntax_tree) and saving intermediate variables. For simplicity, all models are loaded in a single process on a single GPU.
- Convenience: This repo is now `pip`-installable and model weights can be automatically downloaded.

## Installation

You **must** run the following `pip install` command on your machine with an Ampere GPU, as the GLIP model requires CUDA compilation. For simplicity, all models are loaded on a single GPU, so your GPU must also have at least 48G of memory.

```bash
pip install "viper @ git+https://github.com/brown-palm/visual-question-decomposition.git@viper"

# Download models to $TORCH_HOME/hub/viper
python -m viper.download_models
```

## Usage

For further capabilities, please refer to the [API prompt](https://github.com/brown-palm/visual-question-decomposition/blob/main/src/methods/prompts/viper/full_api.txt).

```python
from viper.execution import ViperExecutionModel
viper_model = ViperExecutionModel()

from PIL import Image
image = Image.open('./tree.jpg')

code = """\
def execute_command(image) -> str:
    return ImagePatch(image).find("tree")\
"""

output = viper_model.execute_code(image, code)
result = output['__return__']
print(result)
```

## Notes from the authors

> :warning: WARNING: ViperGPT runs code generated by a large language model. We do not have direct control over this 
> code, so it can be dangerous to run it, especially if modifications to the API are made (the current prompts do not 
> have any dangerous functions like interaction with the filesystem, so it is unlikely that any malicious code can be 
> generated). We cannot guarantee that the code is safe, so use at your own risk, or run in a sandboxed environment.

Note that ViperGPT may inherit biases from the pretrained models it uses. These biases may be reflected in the outputs generated by our model. It is recommended to consider this potential bias when using ViperGPT and interpreting its outputs.
